Resources:
  DataBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: ${self:provider.environment.DATA_BUCKET_NAME}
      PublicAccessBlockConfiguration:
        BlockPublicAcls: True
        BlockPublicPolicy: True
        IgnorePublicAcls: True
        RestrictPublicBuckets: True
      BucketEncryption:
        ServerSideEncryptionConfiguration:
        - ServerSideEncryptionByDefault:
            SSEAlgorithm: 'AES256'
      LoggingConfiguration:
        DestinationBucketName: ${param:loggingBucket}
        LogFilePrefix: s3-data-logs

  DeliveryStreamRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: ${self:provider.environment.NAMESPACE}-delivery-stream-role
      Path: '/anything/roles/'
      Description: 'Role for Firehose to read/write to data bucket'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: ''
            Effect: Allow
            Principal:
              Service: firehose.amazonaws.com
            Action: 'sts:AssumeRole'
            Condition:
              StringEquals:
                'sts:ExternalId': !Ref 'AWS::AccountId'
      Policies:
      # To add permissions to use customer management keys refer to:
      #  https://docs.aws.amazon.com/firehose/latest/dev/controlling-access.html#using-iam-s3
        - PolicyName: ${self:provider.environment.NAMESPACE}-delivery-stream-policy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:AbortMultipartUpload'
                  - 's3:GetBucketLocation'
                  - 's3:GetObject'
                  - 's3:ListBucket'
                  - 's3:ListBucketMultipartUploads'
                  - 's3:PutObject'
                Resource:
                  - !GetAtt DataBucket.Arn
                  - !Sub ${DataBucket.Arn}/*
              - Effect: Allow
                Action:
                  - 'glue:GetTableVersions'
                Resource:
                  # ref: https://docs.aws.amazon.com/glue/latest/dg/glue-specifying-resource-arns.html
                  - !Sub arn:aws:glue:${env:AWS_REGION}:${AWS::AccountId}:catalog
                  - !Sub arn:aws:glue:${env:AWS_REGION}:${AWS::AccountId}:database/${GlueDatabase}
                  - !Sub arn:aws:glue:${env:AWS_REGION}:${AWS::AccountId}:table/${GlueDatabase}/${GlueMainTable}

  # ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-kinesisfirehose-deliverystream.html
  DeliveryStream: 
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties: 
      # Match the source DynamoDB table name, this match is used by lambda-streams-to-firehose handler
      DeliveryStreamName: ${param:streamTableName}
      DeliveryStreamType: DirectPut
      DeliveryStreamEncryptionConfigurationInput:
        KeyType: AWS_OWNED_CMK
      ExtendedS3DestinationConfiguration: 
        BucketARN: !GetAtt DataBucket.Arn
        RoleARN: !GetAtt DeliveryStreamRole.Arn
        Prefix:
          !Join 
            - ''
            - - 'main'
              - '/!{timestamp:YYYY}-!{timestamp:MM}-!{timestamp:dd}/!{timestamp:HH}/'
        ErrorOutputPrefix:
          !Join 
            - ''
            - - 'main'
              - 'error/!{firehose:error-output-type}/!{timestamp:YYYY}-!{timestamp:MM}-!{timestamp:dd}/!{timestamp:HH}/'
        BufferingHints: 
          SizeInMBs: 128
          IntervalInSeconds: 300
        # Must set to uncompressed when using data format conversion. Compression set in the output format config.
        CompressionFormat: UNCOMPRESSED
        S3BackupMode: Disabled
        DataFormatConversionConfiguration:
          # ref: https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html
          SchemaConfiguration:
            CatalogId: !Ref AWS::AccountId
            RoleARN: !GetAtt DeliveryStreamRole.Arn
            DatabaseName: !Ref GlueDatabase
            TableName: !Ref GlueMainTable
            Region: !Ref AWS::Region
            VersionId: LATEST
          InputFormatConfiguration:
            Deserializer:
              #OpenXJsonSerDe: {}
              HiveJsonSerDe:
                TimestampFormats:
                - millis
          OutputFormatConfiguration:
            Serializer:
              #ParquetSerDe:
              #  Compression: GZIP
              OrcSerDe:
                Compression: ZLIB
          Enabled: True

  GlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: ${self:provider.environment.NAMESPACE_ATHENA}
  
  GlueMainTable:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref GlueDatabase
      TableInput:
        Name: main
        Owner: owner
        Retention: 0
        TableType: EXTERNAL_TABLE
        PartitionKeys:
        - Name: year
          Type: string
        - Name: month
          Type: string
        - Name: day
          Type: string
        - Name: hour
          Type: string
        StorageDescriptor:
          Columns:
          - Name: id
            Type: string
          - Name: sort
            Type: string
          - Name: modified
            Type: timestamp
          - Name: author
            Type: string
          - Name: type
            Type: string
          - Name: ctype
            Type: string
          - Name: deleted
            Type: boolean
          - Name: currentVersion
            Type: int
          - Name: eventName
            Type: string
          #- Name: itemIds
          #  Type: array<string>
          # ref: https://github.com/aws/aws-cdk/blob/main/packages/%40aws-cdk/aws-glue-alpha/lib/data-format.ts
          InputFormat: org.apache.hadoop.mapred.TextInputFormat
          #OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
          Compressed: True
          NumberOfBuckets: -1
          SerdeInfo:
            #SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
            SerializationLibrary: org.apache.hadoop.hive.ql.io.orc.OrcSerde
            Parameters:
              serialization.format: '1'
          BucketColumns: []
          SortColumns: []
          StoredAsSubDirectories: False

  QueryResultsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: ${self:provider.environment.NAMESPACE}-queries
      PublicAccessBlockConfiguration:
        BlockPublicAcls: True
        BlockPublicPolicy: True
        IgnorePublicAcls: True
        RestrictPublicBuckets: True
      BucketEncryption:
        ServerSideEncryptionConfiguration:
        - ServerSideEncryptionByDefault:
            SSEAlgorithm: 'AES256'
      LoggingConfiguration:
        DestinationBucketName: ${param:loggingBucket}
        LogFilePrefix: s3-queries-logs
#      LifecycleConfiguration:
#        # Lifecycle rules run once per day; clean up old queries since they're for a live dashboard
#        # TODO If cleanup is enabled, don't remove table (CreateAthenaTable) metadata.
#        #  I.e. just UserActivity/, ItemCountsByType/, Unsaved/ could be wiped daily.
#        Rules:
#          - Id: Daily deletion
#            ExpirationInDays: 1
#            Status: Enabled

  WorkGroup:
    Type: AWS::Athena::WorkGroup
    Properties:
      Name: ${self:provider.environment.NAMESPACE}-workgroup
      State: ENABLED
      WorkGroupConfiguration:
        BytesScannedCutoffPerQuery: 10000000 # 10MB is minimum
        EnforceWorkGroupConfiguration: True
        RequesterPaysEnabled: False
        ResultConfiguration:
          ExpectedBucketOwner: !Ref AWS::AccountId
          OutputLocation: !Sub s3://${QueryResultsBucket}

  # TODO Automatically trigger table creation
  CreateTableQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      WorkGroup: !Ref WorkGroup
      Name: CreateMainTable
      Description: "Define main table"
      QueryString:
        # ref: https://docs.aws.amazon.com/athena/latest/ug/partition-projection-kinesis-firehose-example.html
        !Join 
          - ''
          # >- means replace newlines by space (>) and strip ending newline (-)
          - - >-
              CREATE EXTERNAL TABLE IF NOT EXISTS main (
                id string,
                sort string,
                modified timestamp,
                author string,
                type string,
                ctype string,
                deleted boolean,
                currentVersion int,
                eventName string
              )
              PARTITIONED BY (
                date date,
                hour string
              )
              STORED AS ORC
              LOCATION "${self:provider.environment.DATA_BUCKET_LOCATION}/"
              TBLPROPERTIES (
                "orc.compress" = "ZLIB",
                "projection.enabled" = "true",
                "projection.date.type" = "date",
                "projection.date.format" = "yyyy-MM-dd",
                "projection.date.range" = "2023-01-01,NOW",
                "projection.date.interval" = "1",
                "projection.date.interval.unit" = "DAYS",
                "projection.hour.type" = "integer",
                "projection.hour.range" = "00,23",
                "projection.hour.digits" = "2",
                "storage.location.template" = "${self:provider.environment.DATA_BUCKET_LOCATION}/${date}/${hour}/"
              )

  # Could be a prepared statement with "LIMIT ?" instead.
  ItemCountsByType:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      WorkGroup: !Ref WorkGroup
      Name: ItemCountsByType
      Description: "Types and counts of current items for that type"
      QueryString: >
        SELECT type, count(*) as itemcount
        FROM main
        WHERE sort='v0'
        GROUP BY type ORDER BY itemcount DESC LIMIT 20;

  # Could be prepared statement with "INTERVAL ? DAY" instead.
  # LIMIT should be > unique(author) * 30 days.
  # Cast date as string to just avoid Grafana issues interpreting dates as user's local time zone depending on settings.
  UserActivity:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref GlueDatabase
      WorkGroup: !Ref WorkGroup
      Name: UserActivity
      Description: "Activity (stream events) grouped by date and user"
      QueryString: >
        SELECT cast(date as varchar) as date, coalesce(author, 'system') as author, count(*) as eventcount
        FROM main
        WHERE date > CURRENT_DATE - INTERVAL '30' DAY
        GROUP BY date, author
        ORDER BY date DESC LIMIT 1000;

Outputs:
  dataBucketArn:
    Value: !GetAtt DataBucket.Arn
  queryResultsBucketArn:
    Value: !GetAtt QueryResultsBucket.Arn
  glueDatabase:
    Value: !Ref GlueDatabase
  glueMainTable:
    Value: !Ref GlueMainTable
  athenaWorkgroup:
    Value: !Ref WorkGroup
